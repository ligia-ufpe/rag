# ğŸ“˜ RAG com LangChain e OpenRouter

Este projeto implementa um sistema de **RAG (Retrieval-Augmented Generation)** utilizando [LangChain](https://www.langchain.com/), **FAISS** para indexaÃ§Ã£o vetorial, **embeddings do Hugging Face** e **modelos LLM via OpenRouter**.  

O objetivo Ã© responder perguntas de forma contextualizada com base em um documento PDF.  
Para demonstraÃ§Ã£o, foi utilizado um PDF contendo receitas de bolos caseiros, servindo como exemplo de aplicaÃ§Ã£o prÃ¡tica.

---

## âš™ï¸ Funcionalidades

- ğŸ“‚ **Carregamento de documentos** no formato PDF.  
- âœ‚ï¸ **DivisÃ£o inteligente** do texto em *chunks* com sobreposiÃ§Ã£o para manter o contexto.  
- ğŸ§© **GeraÃ§Ã£o de embeddings vetoriais** utilizando modelos da Hugging Face.  
- ğŸ” **IndexaÃ§Ã£o com FAISS** para busca semÃ¢ntica rÃ¡pida e eficiente.  
- ğŸ¤– **IntegraÃ§Ã£o com modelos LLM** via OpenRouter.  
- âœ… **Respostas sempre baseadas no documento fornecido**, evitando alucinaÃ§Ãµes.  
---

## ğŸ“‚ Estrutura do Projeto
```
â”œâ”€â”€ app.py              # Interface com Streamlit (se usada)
â”œâ”€â”€ rag.py              # NÃºcleo do RAG: carregamento, indexaÃ§Ã£o e resposta
â”œâ”€â”€ receitas_bolos.pdf  # Documento de referÃªncia
â”œâ”€â”€ requirements.txt    # DependÃªncias do projeto
â””â”€â”€ README.md           # Este arquivo
```
---

## ğŸš€ Como Executar

### 1. Instale o Conda

Se ainda nÃ£o tiver o Conda, vocÃª pode instalar o [Miniconda](https://docs.conda.io/en/latest/miniconda.html) (mais leve) ou o [Anaconda](https://www.anaconda.com/download).  

Exemplo de instalaÃ§Ã£o do **Miniconda** no Linux/macOS:  
# Baixe o instalador (Linux)
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
```
# Ou no macOS (Apple Silicon)
```
curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
```
# Instale
```
bash Miniconda3-latest-*.sh
```
No Windows, basta baixar o instalador grÃ¡fico do site oficial e seguir os passos.

Depois da instalaÃ§Ã£o, reinicie o terminal e verifique:
```
conda --version
```

â¸»

2. Crie e ative o ambiente virtual
```
conda create -n rag-env python=3.12 -y
conda activate rag-env
```

â¸»

3. Clone o repositÃ³rio
```
git clone https://github.com/seu-usuario/seu-repo.git
cd seu-repo
```

â¸»

4. Instale as dependÃªncias
```
pip install -r requirements.txt
```

â¸»

5. Configure as variÃ¡veis de ambiente

Crie um arquivo .env na raiz do projeto:
```
OPENROUTER_API_KEY=sk-or-xxxxxxxxxxxxxxxxxxxxxxxx
```

â¸»

6. Execute a aplicaÃ§Ã£o

Se estiver usando apenas o nÃºcleo RAG:
```
python rag.py
```
Se tiver interface Streamlit:
```
streamlit run app.py
```

â¸»

ğŸ§  Principais Componentes

ğŸ”‘ Carregamento e indexaÃ§Ã£o do PDF
```
loader = PyMuPDFLoader("receitas_bolos.pdf")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = splitter.split_documents(docs)

vectorstore = FAISS.from_documents(splits, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

```
ğŸ” RecuperaÃ§Ã£o e resposta
```
def responder_pergunta(pergunta: str) -> str:
    docs = retriever.invoke(pergunta)
    contexto = "\n\n".join([d.page_content for d in docs])
    mensagens = prompt.format_messages(context=contexto, question=pergunta)
    resposta = llm.invoke(mensagens)
    return resposta.content
```

â¸»

##ğŸ“š Tecnologias Utilizadas
- **LangChain** â€“ OrquestraÃ§Ã£o do fluxo RAG.  
- **FAISS** â€“ IndexaÃ§Ã£o e busca vetorial semÃ¢ntica.  
- **Hugging Face Sentence Transformers** â€“ CriaÃ§Ã£o de embeddings.  
- **OpenRouter** â€“ Acesso a modelos LLM.  
- **Streamlit** â€“ Interface interativa (opcional).  
- **Conda** â€“ Gerenciamento de ambientes.  

---

##ğŸ’¡ ObservaÃ§Ãµes
- As respostas sÃ£o sempre extraÃ­das do **PDF fornecido**.  
- Caso a pergunta nÃ£o esteja relacionada ao documento, o modelo responde educadamente que nÃ£o pode responder.  
- O modelo padrÃ£o Ã© **mistralai/mistral-7b-instruct:free**, mas pode ser substituÃ­do por outros disponÃ­veis no catÃ¡logo do **OpenRouter**.  
â¸»

**Autor:** [Gabriel W. A. Matias](https://www.linkedin.com/in/gabriel-w-a-matias-a9913a210/)
